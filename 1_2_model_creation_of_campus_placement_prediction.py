# -*- coding: utf-8 -*-
"""1_2_Model_creation_of_Campus_Placement_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HNTixWKYeeE3lPumGnbFulb50AGRp40Q
"""

df1.to_csv("/content/drive/MyDrive/campus placement/ml-with-python-course-project 2/preprocess_data.csv")

df1

x = df1.drop(['status'],axis = 1)
y = df1.status

x.info()

X_train,x_test,Y_train,y_test = train_test_split(x,y,train_size = 0.8,random_state = 1)

print('y',y.value_counts(),sep='\n')
print()
print('y_train',Y_train.value_counts(),sep='\n')

"""Dataset is imbalanced. Problem is that models trained on imbalanced datasets often have poor results when they have to generalize.

SMOTE resampling on training set
"""

from imblearn.over_sampling import SMOTE

pip install mlrose

import six
import sys
sys.modules['sklearn.externals.six'] = six
import mlrose

pip install mlrose

import sklearn.neighbors._base
sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base

smote= SMOTE(random_state=42)
x_train,y_train = smote.fit_resample(X_train, Y_train)

x_train = pd.DataFrame(x_train)
y_train = pd.DataFrame(y_train)

y_train.value_counts()

y_train

#importing metrics
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score
from sklearn.model_selection import cross_val_score

"""Logistic Regression"""

warnings.filter = warnings.simplefilter('ignore')
logreg = LogisticRegression()
logreg.fit(x_train, y_train)
y_pred = logreg.predict(x_test)

print('Accuracy of logistic regression classifier on test set: {:.3f}'.format(logreg.score(x_test, y_test)))
print('Accuracy of logistic regression classifier on train set: {:.3f}'.format(logreg.score(x_train, y_train)))

"""Hyperparameter Tuning for Logistic Regression"""

param_grid=[{'penalty':['l1','l2','elasticnet','none'],
             'C':np.logspace(-4,4,20),
             'solver':['lbfgs','newton-cg','liblinear','sag','saga'],
             'max_iter':[100,200,1000,2500,5000]}]

from sklearn.model_selection import GridSearchCV
clf=GridSearchCV(logreg,param_grid,cv=3,verbose=True,n_jobs=-1)

best_clf=clf.fit(x_train,y_train)

best_clf.best_estimator_

best_ypred=clf.predict(x_test)

print('Accuracy of logistic regression classifier(GridSearchCV) on test set: {:.3f}'.format(best_clf.score(x_test, y_test)))
print('Accuracy of logistic regression classifier(GridSearchCV) on train set: {:.3f}'.format(best_clf.score(x_train, y_train)))

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test,y_pred)
print("Confusion Matrix:\n",confusion_matrix)
from sklearn.metrics import classification_report
print("Classification Report:\n",classification_report(y_test,y_pred))

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
logit_roc_auc = roc_auc_score(y_test, logreg.predict(x_test))
fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(x_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()

dt = DecisionTreeClassifier()
dt = dt.fit(x_train, y_train)
y_pred_dt= dt.predict(x_test)

print('Accuracy of Decision Tree classifier on test set: {:.3f}'.format(dt.score(x_test, y_test)))
print('Accuracy of Decision Tree classifier on train set: {:.3f}'.format(dt.score(x_train, y_train)))

param_grid_dt={'criterion':['gini','entropy'],'max_depth':[2,3,4,5],'max_features':('auto','sqrt','log2'),'min_samples_split':(2,4,6)}

clf_dt=GridSearchCV(dt,param_grid_dt,n_jobs=-1,verbose=True,cv=5)

clf_dt.fit(x_train,y_train)

clf_dt.best_estimator_

best_ypred_dt=clf_dt.predict(x_test)

print('Accuracy of Decision Tree(GridSearchCV) classifier on test set: {:.3f}'.format(clf_dt.score(x_test, y_test)))
print('Accuracy of Decision Tree(GridSearchCV) classifier on train set: {:.3f}'.format(clf_dt.score(x_train, y_train)))

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test,best_ypred_dt)
print("Confusion Matrix:\n",confusion_matrix)
print("Classification Report:\n",classification_report(y_test,best_ypred_dt))

rf = RandomForestClassifier(n_estimators=100)
rf.fit(x_train,y_train)
y_pred_rf = rf.predict(x_test)

print('Accuracy of Random Forest classifier on test set: {:.3f}'.format(rf.score(x_test, y_test)))
print('Accuracy of Random Forest classifier on train set: {:.3f}'.format(rf.score(x_train, y_train)))

# Number of trees in random forest
n_estimators =[20,60,100,120]
# Number of features to consider at every split
max_features = [0.2,0.6,1.0]
# Maximum number of levels in tree
max_depth = [2,4,8,None]
# Minimum number of samples required to split a node
min_samples_split = [2, 5]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2]
# Method of selecting samples for training each tree
bootstrap = [True, False]
criterion=['gini','entropy']

param_grid_rf = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap,'criterion':criterion}

clf_rf = GridSearchCV(estimator = rf, param_grid = param_grid_rf, cv = 5, verbose=2, n_jobs = -1)

clf_rf.fit(x_train, y_train)

clf_rf.best_estimator_

best_ypred_rf=clf_rf.predict(x_test)

print('Accuracy of Random Forest(GridSearchCV) classifier on test set: {:.3f}'.format(clf_rf.score(x_test, y_test)))
print('Accuracy of Random Forest(GridSearchCV) classifier on train set: {:.3f}'.format(clf_rf.score(x_train, y_train)))

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test,best_ypred_rf)
print("Confusion Matrix:\n",confusion_matrix)
print("Classification Report:\n",classification_report(y_test,best_ypred_rf))

from sklearn.naive_bayes import BernoulliNB 
nb = BernoulliNB() 
nb.fit(x_train, y_train) 
  
#Applying and predicting 
y_pred_nb = nb.predict(x_test) 

print('Accuracy of Naive Bayes classifier on test set: {:.3f}'.format(nb.score(x_test, y_test)))
print('Accuracy of Naive Bayes classifier on train set: {:.3f}'.format(nb.score(x_train, y_train)))

"""Hyperparameter Tuning for Naive Bayes"""

para_gird_nb={'alpha':[0.01,0.1,0.5,1,10]}

clf_nb=GridSearchCV(nb,para_gird_nb,n_jobs=-1,cv=5,verbose=5)

clf_nb.fit(x_train,y_train)

clf_nb.best_estimator_

best_ypred_nb=clf_nb.predict(x_test)

print('Accuracy of Naive Bayes classifier on test set(GridSearchCV): {:.3f}'.format(clf_nb.score(x_test, y_test)))
print('Accuracy of Naive Bayes classifier on train set(GridSearchCV): {:.3f}'.format(clf_nb.score(x_train, y_train)))

"""Confusion matrix and Classification report"""

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred_nb)
print("Confusion Matrix:\n",confusion_matrix)
print("Classification Report:\n",classification_report(y_test, y_pred_nb))

"""SVM"""

from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC
svclassifier = SVC(kernel='linear',probability=True)
svclassifier.fit(x_train, y_train)
y_pred_svm = svclassifier.predict(x_test)

print('Accuracy of SVM  on test set: {:.3f}'.format(svclassifier.score(x_test, y_test)))
print('Accuracy of SVM  on train set: {:.3f}'.format(svclassifier.score(x_train, y_train)))

confusion_matrix = confusion_matrix(y_test,y_pred_svm)
print("Confusion Matrix:\n",confusion_matrix)
print("Classification Report:\n",classification_report(y_test,y_pred_svm))

svm_roc_auc = roc_auc_score(y_test, svclassifier.predict(x_test))
fpr, tpr, thresholds = roc_curve(y_test, svclassifier.predict_proba(x_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='SVM (area = %0.2f)' % svm_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()

import pickle

with open('/content/drive/MyDrive/campus placement/model_pickle','wb') as f:
  pickle.dump(clf,f)

with open('/content/drive/MyDrive/campus placement/model_pickle','rb') as f:
  map = pickle.load(f)

map

map.predict(x_test.values)



from prettytable import PrettyTable


x = PrettyTable()
x.field_names =  ['MODEL','TRAIN_ACCURACY','TEST_ACCURACY']

x.add_row(["Log Reg ", 'Log reg train = 0.896','Log reg test = 0.907'])

x.add_row(["Hyperparameter Tuning Log Reg ", 'Log reg(GridSearchCV)train = 0.908','Log reg(GridSearchCV) test = 0.907'])

x.add_row(["DT", 'DT train = 1.000','DT test = 0.791'])

x.add_row(["Hyperparam Tuning for DT", 'DT(GridSearchCV) train = 0.908','DT(GridSearchCV)test = 0.721'])

x.add_row(["RF", 'RF train = 1.000','RF test = 0.884'])

x.add_row(["Hyperparam Tuning for RF", 'RF(GridSearchCV) train = 1.000','RF(GridSearchCV) test = 0.814'])

x.add_row(["NB BernoulliNB", 'NB train = 0.762','NB test = 0.698'])

x.add_row(["Hyperparameter Tuning for NB", 'NB train (GridSearchCV)= 0.758','NB test set(GridSearchCV): 0.698'])

x.add_row(["SVM", 'Accuracy = SVM train = 0.912','Accuracy = SVM test = 0.907'])












print('\n')
print(x)

df

!python --version

pip freeze>"/content/drive/MyDrive/campus placement/requirements.txt"

"""Conclusion Comparing all the applied models, it is evident the Logistic Regression with GridSearchCV is performing the best with a precision and accuracy score of 90.8 and 90.7 respectively. """